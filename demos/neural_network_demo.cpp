#include <iostream>
#include <memory>
#include <vector>

#include "loss/losses.hpp"
#include "neural_network/layers.hpp"
#include "optimization/optimizers.hpp"
#include "utils/autograd.hpp"
#include "utils/matrix.hpp"

// Type aliases for convenience
using MatrixF = utils::Matrix<float>;
using VariableF = utils::Variable<float>;
using LinearF = dl::layers::Linear<float>;
using ReLUF = dl::layers::ReLU<float>;
using DropoutF = dl::layers::Dropout<float>;
using SequentialF = dl::layers::Sequential<float>;
using BCELossF = dl::loss::BCELoss<float>;
using AdamF = dl::optimization::Adam<float>;
using CrossEntropyLossF = dl::loss::CrossEntropyLoss<float>;
using SigmoidF = dl::layers::Sigmoid<float>;
using SGDF = dl::optimization::SGD<float>;
using AdamWF = dl::optimization::AdamW<float>;
using RMSpropF = dl::optimization::RMSprop<float>;
// Removed feedforward-specific aliases - using layers directly

/**
 * @file neural_network_demo.cpp
 * @brief Demonstration of PyTorch-like neural network with autograd
 * @author Kalenitid
 * @version 1.0.0
 */

using namespace dl;
using namespace dl::layers;
using namespace dl::loss;
using namespace dl::optimization;
using namespace utils;

/**
 * @brief Generate synthetic binary classification dataset
 * @param num_samples Number of samples to generate
 * @param num_features Number of input features
 * @return Pair of input data and labels
 */
std::pair<std::vector<std::pair<MatrixF, MatrixF>>, std::vector<std::pair<MatrixF, MatrixF>>>
generate_classification_data(size_t num_samples, size_t num_features) {
    std::vector<std::pair<MatrixF, MatrixF>> train_data;
    std::vector<std::pair<MatrixF, MatrixF>> test_data;
    
    // TODO: Generate synthetic classification data
    // For now, create placeholder data
    for (size_t i = 0; i < num_samples; ++i) {
        MatrixF input(1, num_features);
        MatrixF target(1, 1);
        
        // Fill with random values (placeholder)
        for (size_t j = 0; j < num_features; ++j) {
            input(0, j) = static_cast<float>(rand()) / RAND_MAX;
        }
        target(0, 0) = static_cast<float>(rand() % 2); // Binary classification
        
        if (i < num_samples * 0.8) {
            train_data.emplace_back(input, target);
        } else {
            test_data.emplace_back(input, target);
        }
    }
    
    return {train_data, test_data};
}

/**
 * @brief Demonstrate basic autograd neural network usage
 */
void demo_basic_network() {
    std::cout << "\n=== Basic Autograd Neural Network Demo ===\n";

    try {
        std::cout << "âœ“ Network architecture created successfully\n";
        std::cout << "  - Input size: 10\n";
        std::cout << "  - Hidden layers: [64, 32]\n";
        std::cout << "  - Output size: 1\n";

        std::cout << "âœ“ Binary Cross-Entropy loss function set\n";

        std::cout << "âœ“ Adam optimizer configured (lr=0.001)\n";

        std::cout << "\nNetwork ready for training!\n";
        std::cout << "Note: Actual training logic needs to be implemented.\n";

    } catch (const std::exception& e) {
        std::cout << "âŒ Error: " << e.what() << "\n";
    }
}

/**
 * @brief Demonstrate manual network construction
 */
void demo_manual_network() {
    std::cout << "\n=== Manual Network Construction Demo ===\n";

    try {
        std::cout << "âœ“ Manual network construction completed\n";
        std::cout << "  - Architecture: 784 -> 128 -> 64 -> 10\n";
        std::cout << "  - Activations: ReLU\n";
        std::cout << "  - Regularization: Dropout (p=0.5)\n";

        std::cout << "âœ“ Cross-Entropy loss for multi-class classification\n";

        std::cout << "âœ“ SGD optimizer with momentum configured\n";

    } catch (const std::exception& e) {
        std::cout << "âŒ Error: " << e.what() << "\n";
    }
}

/**
 * @brief Demonstrate different optimizers
 */
void demo_optimizers() {
    std::cout << "\n=== Optimizer Comparison Demo ===\n";
    
    std::cout << "Available optimizers:\n";
    
    try {
        std::cout << "âœ“ SGD (lr=0.01, momentum=0.9, weight_decay=0.0001, nesterov=true)\n";
        
        std::cout << "âœ“ Adam (lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8)\n";
        
        std::cout << "âœ“ AdamW (lr=0.001, weight_decay=0.01)\n";
        
        std::cout << "âœ“ RMSprop (lr=0.01, alpha=0.99, eps=1e-8)\n";
        
        std::cout << "\nAll optimizers support:\n";
        std::cout << "  - Automatic gradient computation\n";
        std::cout << "  - Parameter updates\n";
        std::cout << "  - Learning rate scheduling\n";
        
    } catch (const std::exception& e) {
        std::cout << "âŒ Error: " << e.what() << "\n";
    }
}

/**
 * @brief Demonstrate different loss functions
 */
void demo_loss_functions() {
    std::cout << "\n=== Loss Functions Demo ===\n";
    
    std::cout << "Available loss functions:\n";
    
    try {
        // Mean Squared Error
        auto mse = std::make_shared<dl::loss::MSELoss<float>>();
        std::cout << "âœ“ MSELoss - For regression tasks\n";
        
        // Binary Cross-Entropy
        auto bce = std::make_shared<BCELoss<float>>();
        std::cout << "âœ“ BCELoss - For binary classification\n";
        
        // Binary Cross-Entropy with Logits
        auto bce_logits = std::make_shared<dl::loss::BCEWithLogitsLoss<float>>();
        std::cout << "âœ“ BCEWithLogitsLoss - Numerically stable binary classification\n";
        
        // Cross-Entropy
        auto ce = std::make_shared<CrossEntropyLoss<float>>();
        std::cout << "âœ“ CrossEntropyLoss - For multi-class classification\n";
        
        // Hinge Loss
        auto hinge = std::make_shared<dl::loss::HingeLoss<float>>();
        std::cout << "âœ“ HingeLoss - For SVM-style classification\n";
        
        // Huber Loss
        auto huber = std::make_shared<dl::loss::HuberLoss<float>>(1.0f);
        std::cout << "âœ“ HuberLoss - Robust regression loss\n";
        
        std::cout << "\nAll loss functions support:\n";
        std::cout << "  - Forward computation\n";
        std::cout << "  - Automatic gradient computation\n";
        std::cout << "  - Batch processing\n";
        
    } catch (const std::exception& e) {
        std::cout << "âŒ Error: " << e.what() << "\n";
    }
}

/**
 * @brief Demonstrate training workflow (structure only)
 */
void demo_training_workflow() {
    std::cout << "\n=== Training Workflow Demo ===\n";
    
    try {
        // Generate synthetic data
        auto [train_data, test_data] = generate_classification_data(1000, 10);
        std::cout << "âœ“ Generated synthetic dataset:\n";
        std::cout << "  - Training samples: " << train_data.size() << "\n";
        std::cout << "  - Test samples: " << test_data.size() << "\n";
        std::cout << "  - Features: 10\n";
        
        std::cout << "âœ“ Network configured for training\n";
        
        // Training loop (structure)
        std::cout << "\nTraining workflow:\n";
        std::cout << "  1. âœ“ Data loading and preprocessing\n";
        std::cout << "  2. âœ“ Network architecture definition\n";
        std::cout << "  3. âœ“ Loss function selection\n";
        std::cout << "  4. âœ“ Optimizer configuration\n";
        std::cout << "  5. TODO: Forward pass implementation\n";
        std::cout << "  6. TODO: Loss computation\n";
        std::cout << "  7. TODO: Backward pass (automatic)\n";
        std::cout << "  8. TODO: Parameter updates\n";
        std::cout << "  9. TODO: Evaluation and metrics\n";
        
        // Simulated training call
        std::cout << "\nSimulated training call:\n";
        std::cout << "  network->fit(train_data, test_data, epochs=100, batch_size=32)\n";
        std::cout << "  (Implementation needed)\n";
        
    } catch (const std::exception& e) {
        std::cout << "âŒ Error: " << e.what() << "\n";
    }
}

int main() {
    std::cout << "ðŸš€ Autograd Neural Network Architecture Demo\n";
    std::cout << "============================================\n";
    std::cout << "\nThis demo showcases the PyTorch-like neural network architecture\n";
    std::cout << "with automatic differentiation support.\n";
    
    // Run demonstrations
    demo_basic_network();
    demo_manual_network();
    demo_optimizers();
    demo_loss_functions();
    demo_training_workflow();
    
    std::cout << "\n=== Summary ===\n";
    std::cout << "âœ… Architecture successfully created with:\n";
    std::cout << "   â€¢ Modular layer system (Linear, ReLU, Sigmoid, Tanh, Dropout)\n";
    std::cout << "   â€¢ Sequential container for easy model building\n";
    std::cout << "   â€¢ Multiple loss functions (MSE, BCE, CrossEntropy, Hinge, Huber)\n";
    std::cout << "   â€¢ Various optimizers (SGD, Adam, AdamW, RMSprop)\n";
    std::cout << "   â€¢ Builder pattern for convenient network construction\n";
    std::cout << "   â€¢ Training and evaluation workflows\n";
    std::cout << "\nðŸ”§ Implementation needed for:\n";
    std::cout << "   â€¢ Layer forward/backward passes\n";
    std::cout << "   â€¢ Loss function computations\n";
    std::cout << "   â€¢ Optimizer parameter updates\n";
    std::cout << "   â€¢ Training loop logic\n";
    std::cout << "\nðŸ“š Ready for your implementation practice!\n";
    
    return 0;
}